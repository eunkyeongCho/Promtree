# DB
from pymongo.errors import ConnectionFailure
from pymongo import MongoClient

# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
import re
from urlextract import URLExtract
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ì˜ˆì™¸ì²˜ë¦¬
import traceback

# ê¸°íƒ€
from dotenv import load_dotenv
import os
from pathlib import Path


# =====================================================
# DB ì»¤ë„¥íŒ… (setup.ipynb -> pyíŒŒì¼ë¡œ ë°”ë€Œë©´ ê·¸ê±° importí•˜ê³  ì´ ë¶€ë¶„ì€ ì‚­ì œë  ì˜ˆì •)
# =====================================================
load_dotenv() # í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°

USERNAME = os.getenv("MONGODB_INITDB_ROOT_USERNAME") # í™˜ê²½ë³€ìˆ˜ì— ìˆë˜ ê°’ë“¤ ë©”ëª¨ë¦¬ ë³€ìˆ˜ì— í• ë‹¹
PASSWORD = os.getenv("MONGODB_INITDB_ROOT_PASSWORD")
HOST = os.getenv("MONGO_HOST")
PORT = int(os.getenv("MONGO_PORT"))

url = f"mongodb://{USERNAME}:{PASSWORD}@{HOST}:{PORT}/" # ìœ„ ê°’ë“¤ ì¡°í•©í•´ì„œ MongoDB ì—°ê²°í•˜ê¸° ìœ„í•œ url ë§Œë“¤ê¸°
print(url) # ë§Œë“¤ì–´ì§„ url ì¶œë ¥

try:
    client = MongoClient(url)
    client.admin.command('ping')
    print("Successfully connected to MongoDB!")

except ConnectionFailure as e:
    print(f"MongoDB connection failed: {e}")

# chunkì— ì“¸ database, collection ë³€ìˆ˜ ì„ ì–¸ ë° í• ë‹¹
# ì•„ì§ ë„¤ì´ë° í˜‘ì˜ ì „ì´ë¯€ë¡œ ê°ì ì‹¤í–‰í•˜ëŠ” ë¡œì»¬ì—ì„œì˜ db, collection ì´ë¦„ê³¼ ê²¹ì¹˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸í•„ìš”
chunk_db = client['chunk_db']
chunk_collection = chunk_db['chunk_collection']

# =====================================================

def get_pages_info(md: str) -> list:
    """
    ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ì—ì„œ ê° í˜ì´ì§€ì˜ ì‹¤ì œ ë‚´ìš©ì˜ ì‹œì‘ê³¼ ë ì¸ë±ìŠ¤ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        md(str): ì²˜ë¦¬í•´ì•¼í•  ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë¬¸ìì—´

    Returns: list[tuple[int, tuple[int, int]]]: ê° í˜ì´ì§€ì˜ ì¸ë±ìŠ¤ ì •ë³´ë¥¼ ë‹´ì€ ë¦¬ìŠ¤íŠ¸.
        í˜•ì‹: [[page_num, (content_start_index, content_end_index)]]
        ì„¤ëª…:
            - page_num (int): í˜ì´ì§€ ë²ˆí˜¸
            - content_start_index (int): '>>> page' ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” ì‹¤ì œ ë‚´ìš©ì˜ ì‹œì‘ ì¸ë±ìŠ¤.
            - content_end_index (int): '>>> pend' ì§ì „ì— ëë‚˜ëŠ” ì‹¤ì œ ë‚´ìš©ì˜ ë ì¸ë±ìŠ¤.
    """

    # >>> page_0 ... ì •ê·œì‹ (ê·¸ ë‹¤ìŒ >>> page_1ì´ ë‚˜ì˜¤ê¸° ì§ì „ê¹Œì§€)
    page_pattern = re.compile(
        r">>> page_(\d+)(.*?)(?=>>> page_\d+|$)",
        re.DOTALL
    )

    pages_info = [] # ê° í˜ì´ì§€ì—ì„œ >>> page_0ì„ ì œì™¸í•œ ì‹¤ì œ ë‚´ìš©ì˜ ì‹œì‘ê³¼ ë ì¸ë±ìŠ¤ ì •ë³´ë¥¼ ì €ì¥í•  ë°°ì—´

    for page_match in page_pattern.finditer(md): # mdì—ì„œ ì •ê·œì‹ì´ ë§¤ì¹­ë  ë•Œ ë§ˆë‹¤ re.Match ê°ì²´ë¥¼ ë°˜í™˜
        
        is_last_page = not re.match(r">>> page_\d+", md[page_match.end(2):]) # ë§ˆì§€ë§‰ í˜ì´ì§€ì¸ì§€ ì—¬ë¶€ë¥¼ íŒë‹¨

        content_start_index = page_match.start(2) # page ë©ì–´ë¦¬ ì‹œì‘ ì¸ë±ìŠ¤

        if is_last_page:
            content_end_index = len(md)
        else:
            content_end_index = page_match.end(2) - 1 # page ë©ì–´ë¦¬ ë ì¸ë±ìŠ¤ (re.Match.end() ì¸ë±ìŠ¤ê°€ ë ì¸ë±ìŠ¤ì˜ ë‹¤ìŒ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•˜ê¸° ë•Œë¬¸ì— -1)
        
        page_num = int(page_match.group(1)) # í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸

        pages_info.append([page_num, (content_start_index, content_end_index)])

    return pages_info

def remove_page(md: str) -> str:
    """
    ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ì—ì„œ '>>> page_0' ë§ˆì»¤ë¥¼ ì œê±°í•˜ê³  ê·¸ ë§Œí¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜í•©ë‹ˆë‹¤.
    ì œê±°í•œ ë§Œí¼ ê³µë°±ìœ¼ë¡œ ì±„ìš°ê¸° ë•Œë¬¸ì— ì›ë³¸ ì¸ë±ìŠ¤ê°€ ë³´ì¡´ë©ë‹ˆë‹¤.

    Args:
        md(str): ì²˜ë¦¬í•´ì•¼í•  ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë¬¸ìì—´ (pdf íŒŒì¼ 1ê°œ ë‹¨ìœ„)

    Returns:
        '>>> page_0' ë§ˆì»¤ê°€ ì œê±°ëœ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë¬¸ìì—´
    """

    page_pattern = re.compile(
        r">>> page_\d+",
        re.DOTALL
    )

    md_without_page = page_pattern.sub(lambda m: " " * len(m.group(0)), md)

    return md_without_page

def generate_image_chunk(md: str) -> dict[str, str | list[dict]]:
    """
    ë§ˆí¬ë‹¤ìš´ ë¬¸ìì—´ì—ì„œ imageì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì„ ì „ë¶€ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜í•˜ê³  image íƒ€ì… ì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        md(str): >>> page, >>> pendê°€ ì œê±°ëœ ë§ˆí¬ë‹¤ìš´ ë¬¸ìì—´

    Returns:
        dict: ë‹¤ìŒ ë‘ ê°œì˜ keyë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
            - md_without_image(str): ëª¨ë“  ì´ë¯¸ì§€ì§€ê°€ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜ëœ ë§ˆí¬ë‹¤ìš´ ë¬¸ìì—´
            - image_raw_chunks(list[dict]): image íƒ€ì… raw ì²­í¬ ë¦¬ìŠ¤íŠ¸ (file_name, page_num í‚¤ ìƒì„± ì „)
    """

    # image ì •ê·œì‹
    image_pattern = re.compile(
        r"!\[(.*?)\]\((.*?)\)",
        re.DOTALL
    )

    image_raw_chunks = []

    for image_match in re.finditer(image_pattern, md):
        image_raw_chunks.append({
                "type": "image",
                "content": image_match.group(2),
                "metadata": image_match.group(1),
                "start_index": image_match.start(),
                "end_index": image_match.end() - 1
            })
        
    # image ë¶€ë¶„ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜
    md_without_image = image_pattern.sub(lambda m: " " * len(m.group(0)), md)

    image_dict = {
        "md_without_image": md_without_image,
        "image_raw_chunks": image_raw_chunks
    }
    
    return image_dict

def generate_link_chunk(md: str) -> dict[str, str | list[dict]]:
    """
    ë§ˆí¬ë‹¤ìš´ ë¬¸ìì—´ì—ì„œ linkì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì„ ì „ë¶€ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜í•˜ê³  link íƒ€ì… ì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        md(str): imageê°€ ì œê±°ëœ ë§ˆí¬ë‹¤ìš´ ë¬¸ìì—´

    Returns:
        dict: ë‹¤ìŒ ë‘ ê°œì˜ keyë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
            - md_without_link(str): ëª¨ë“  ë§í¬ê°€ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜ëœ ë§ˆí¬ë‹¤ìš´ ë¬¸ìì—´
            - link_raw_chunks(list[dict]): link íƒ€ì… ì²­í¬ ë¦¬ìŠ¤íŠ¸ (file_name, page_num í‚¤ ìƒì„± ì „)
    """

    # link ì°¾ê¸°
    link_extractor = URLExtract()
    links = link_extractor.find_urls(md)

    link_raw_chunks = [] # link íƒ€ì… raw chunkë“¤ ì €ì¥í•  ë°°ì—´ (page_num, file_nameì´ ë¶™ê¸° ì „)

    for link in links: # ì°¾ì€ ë§í¬ë“¤ì— ëŒ€í•´ í•´ë‹¹ ë¶€ë¶„ì„ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜í•˜ê³  ì²­í¬ ìƒì„±í•´ì„œ chunk_collectionì— ì €ì¥

        link_pattern = re.compile(re.escape(link)) # í˜„ì¬ linkë¥¼ ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œ

        for link_match in re.finditer(link_pattern, md):

            link_start = link_match.start()
            link_end = link_match.end()

            # linkì— ëŒ€í•œ metadataë¡œ ì“¸ ë¬¸ë§¥ ì¶”ì¶œ
            context_start = max(0, link_start - 100)
            context_end = min(len(md), link_end + 100)
            context_snippet = md[context_start:link_start] + md[link_end:context_end]

            # ì²­í¬ë¡œ ë§Œë“¤ì–´ì„œ link_raw_chunks ë°°ì—´ì— ì €ì¥
            link_raw_chunks.append({
                "type": "link",
                "content": link,
                "metadata": context_snippet,
                "start_index": context_start, # page_num ë§Œë“œëŠ” í•¨ìˆ˜ í†µê³¼í•˜ë©´ì„œ ì—†ì–´ì§ˆ í‚¤
                "end_index": context_end # ì´ê²ƒë„ ë§ˆì°¬ê°€ì§€
            })

        # link ë¶€ë¶„ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜ (ì´ì „ ë£¨í”„ì˜ linkê°€ ì œê±°ëœ mdê°€ ë‹¤ìŒ ë£¨í”„ë¡œ ì „ë‹¬ë¨)
        md = link_pattern.sub(lambda m: " " * len(m.group(0)), md)

    link_dict = {
        "md_without_link": md,
        "link_raw_chunks": link_raw_chunks
    }
    
    return link_dict

def generate_table_chunk(md: str) -> dict[str, str | list[dict]]:
    """
    ë§ˆí¬ë‹¤ìš´ ë¬¸ìì—´ì—ì„œ tableì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì„ ì „ë¶€ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜í•˜ê³  table íƒ€ì… ì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ê°™ì€ íŒŒì¼ ë‚´ì— tableì´ ì—¬ëŸ¬ê°œ ìˆëŠ” ê²½ìš° ê° table ë³„ë¡œ ì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    ë‹¨, í…Œì´ë¸”ê³¼ í…Œì´ë¸” ì‚¬ì´ì— ê·¸ ì–´ë–¤ ë¬¸ìë„ ì—†ê³  ë°”ë¡œ ë‹¤ìŒ í…Œì´ë¸”ì´ ë“±ì¥í•˜ëŠ” ê²½ìš°ì—ëŠ” ë‘ê°œì˜ í…Œì´ë¸”ì´ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ë¬¶ì´ê²Œ ë©ë‹ˆë‹¤.
    ì´ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ëŠ” ë¡œì§ì´ ì–´ë ¤ì›Œì„œ ì•„ì§ ë§Œë“¤ì§€ ëª»í–ˆëŠ”ë° ì¶”í›„ ë³´ê°•í•  ìˆ˜ ìˆë‹¤ë©´ ë³´ê°•í•˜ê² ìŠµë‹ˆë‹¤.

    tableì— í•´ë‹¹í•˜ëŠ” ë§ˆí¬ë‹¤ìš´ ë¬¸ìì—´ì—ì„œ |ì™€ -ë¥¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜í•©ë‹ˆë‹¤. ë˜í•œ, í•œ tableì— ì†í•˜ëŠ” ë‚´ìš©ì„ ì—¬ëŸ¬ ì²­í¬ë¡œ ë‚˜ëˆ„ì§€ ì•Šê³  í•˜ë‚˜ì˜ ì²­í¬ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        md(str): linkê°€ ì œê±°ëœ ë§ˆí¬ë‹¤ìš´ ë¬¸ìì—´

    Returns:
        dict: ë‹¤ìŒ ë‘ ê°œì˜ keyë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
            - md_without_table(str): ëª¨ë“  í‘œê°€ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜ëœ ë§ˆí¬ë‹¤ìš´ ë¬¸ìì—´
            - table_raw_chunks(list[dict]): table íƒ€ì… ì²­í¬ ë¦¬ìŠ¤íŠ¸ (file_name, page_num í‚¤ ìƒì„± ì „)
    """

    # table ì •ê·œì‹
    table_pattern = re.compile(
        r"((?:^\s*\|.*\n)+)", # |ë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸ì´ ë°œê²¬ë˜ë©´, |ë¡œ ì‹œì‘í•˜ì§€ ì•ŠëŠ” ë¼ì¸ì´ ë‚˜ì˜¬ ë•Œ ê¹Œì§€ ê³„ì†í•´ì„œ í•œ ë©ì–´ë¦¬ë¡œ í•©ì³ì„œ ë§¤ì¹­
        re.MULTILINE
    )

    table_raw_chunks = [] # link íƒ€ì… raw chunkë“¤ ì €ì¥í•  ë°°ì—´ (page_num, file_nameì´ ë¶™ê¸° ì „)

    for table_match in re.finditer(table_pattern, md):

        table_str = table_match.group(0)
        table_start_index = table_match.start()
        table_end_index = table_match.end() - 1

        replaced_table_str = re.sub(r"[|\-]", " ", table_str) # | ë˜ëŠ” -ì„ ì „ë¶€ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜

        # ì²­í¬ë¡œ ë§Œë“¤ì–´ì„œ table_chunks ë°°ì—´ì— ì €ì¥
        table_raw_chunks.append({
            "type": "table",
            "content": replaced_table_str,
            "start_index": table_start_index,
            "end_index": table_end_index
        })

    # table ë¶€ë¶„ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜
    md = table_pattern.sub(lambda m: " " * len(m.group(0)), md)

    table_dict = {
        "md_without_table": md,
        "table_raw_chunks": table_raw_chunks
    }
        
    return table_dict

def generate_text_chunk(md: str) -> list:
    """
    ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë¬¸ìì—´ì— Header(# ~ ######)ê°€ í¬í•¨ëëŠ”ì§€ ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°í•´ì„œ text íƒ€ì… ì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Headerê°€ í¬í•¨ëœ ë¬¸ìì—´ì´ë¼ë©´, ê°™ì€ header pathë¥¼ ê³µìœ í•˜ëŠ” í…ìŠ¤íŠ¸ ë‹¨ìœ„ë¡œ ì²­í‚¹í•©ë‹ˆë‹¤.
    ê° ì²­í¬ ì•ì—ëŠ” í•´ë‹¹ ì²­í¬ì˜ header path ë¬¸ìì—´ì„ ê²°í•©í•´ì„œ retrieve ì‹œ ê° ì²­í¬ì— ëŒ€í•´ ë§¥ë½ì •ë³´ê°€ íš¨ê³¼ì ìœ¼ë¡œ ë°˜ì˜ë˜ë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤.
    header pathë€, ê° ì²­í¬ê°€ ì†í•œ headerê°€ ì†í•œ ìµœìƒìœ„ headerê¹Œì§€ì˜ ëª¨ë“  headerë¥¼ ê³µë°± í•˜ë‚˜ë¥¼ ì‚¬ì´ì— ë‘ê³  ì´ì–´ë¶™ì¸ ë¬¸ìì—´ì…ë‹ˆë‹¤.

    Headerê°€ í¬í•¨ë˜ì§€ ì•Šì€ ë¬¸ìì—´ì´ë¼ë©´, Recursive Chunking í•©ë‹ˆë‹¤.
    ì²­í¬ í•˜ë‚˜ì˜ í¬ê¸°ëŠ” 1000ì, overlap í¬ê¸°ëŠ” 200ì ì…ë‹ˆë‹¤.

    Args:
        md(str): tableì´ ì œê±°ëœ ë§ˆí¬ë‹¤ìš´ ë¬¸ìì—´

    Returns:
        list: text íƒ€ì… ì²­í¬ ë¦¬ìŠ¤íŠ¸ (file_name, page_num í‚¤ ìƒì„± ì „)
    """

    header_pattern = re.compile(r"^#{1,6}\s", re.MULTILINE) # header íŒ¨í„´
    has_header = bool(header_pattern.search(md)) # header ì¡´ì¬ì—¬ë¶€ íŒë‹¨

    text_raw_chunks = [] # text íƒ€ì… raw chunkë“¤ ì €ì¥í•  ë°°ì—´ (page_num, file_nameì´ ë¶™ê¸° ì „)

    if has_header: # headerê°€ ìˆëŠ” ë¬¸ì„œì¸ ê²½ìš°

        header_hierarchy = [] # Header ë¬¸ì¥ì˜ ì•ì— # ë¶€ë¶„ë§Œ ì €ì¥
        header_titles = [] # Header ë¬¸ì¥ì˜ # ì œì™¸ ì œëª© ë¶€ë¶„ë§Œ ì €ì¥
        texts = {} # í…ìŠ¤íŠ¸ë¥¼ ëˆ„ì í•´ì„œ ì €ì¥í•  ë¬¸ìì—´ (header pathê°€ ë‹¬ë¼ì§€ë©´ ì´ˆê¸°í™”)

        # \nìœ¼ë¡œ ëë‚˜ëŠ” ë¬¸ìì—´(ë¬¸ì¥)ì„ ë§¤ì¹­í•˜ëŠ” ì •ê·œì‹
        line_pattern = re.compile(
            r".*?\n|.*$",
            re.DOTALL)

        header_capture_pattern = re.compile(r"^(#{1,6})\s(.*)") # header ì •ë³´ ìº¡ì³ìš© íŒ¨í„´

        for line_match in re.finditer(line_pattern, md):

            line = line_match.group(0) # í˜„ì¬ lineì˜ ë‚´ìš©

            if not line.strip():
                continue

            header_match = header_capture_pattern.match(line) # Headerë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ì¸ì§€ ì •ê·œì‹ ì ìš©

            if header_match: # Headerë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ì´ë¼ë©´
                
                if texts: # ì—¬íƒœ ëˆ„ì ë¼ìˆëŠ” textsê°€ ìˆë‹¤ë©´ raw text chunk ì œì‘í•´ì„œ append
                    text_raw_chunks.append({
                        "type": "text",
                        "content": texts['content'],
                        "start_index": texts['start_index'],
                        "end_index": texts['end_index']
                    })

                    texts = {} # texts ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”

                hashs = header_match.group(1).strip() # í˜„ì¬ hashs
                title = header_match.group(2).strip() # í˜„ì¬ title

                current_level = len(hashs) # í˜„ì¬ level

                while header_hierarchy: # header_hierarchy ë°°ì—´ì´ ë¹„ì–´ìˆì§€ ì•Šì„ ë™ì•ˆ ë°˜ë³µ

                    if len(header_hierarchy[-1]) >= current_level: # header_hierarchy ë°°ì—´ì˜ ëë¶€í„° ê²€ì‚¬í•˜ë©´ì„œ í˜„ì¬ ë ˆë²¨ë³´ë‹¤ ê°™ê±°ë‚˜ ë‚®ì€ ë ˆë²¨ì´ë¼ë©´ pop
                        header_hierarchy.pop()
                        header_titles.pop()

                    else: # í˜„ì¬ ë ˆë²¨ë³´ë‹¤ ë†’ë‹¤ë©´ append
                        break

                header_hierarchy.append(hashs)
                header_titles.append(title)

            else: # Headerë¡œ ì‹œì‘í•˜ì§€ ì•ŠëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸ ë¬¸ì¥ì´ë¼ë©´

                if not texts: # ê¸°ì¡´ì— ìŒ“ì¸ textê°€ ì—†ë‹¤ë©´ header path ë§Œë“¤ì–´ì„œ í˜„ì¬ ë¬¸ì¥ ë‚´ìš©ì´ë‘ ë¬¸ìì—´ ê²°í•©
                    header_path = " > ".join(header_titles) # í˜„ì¬ header path

                    # header pathì™€ ê²°í•©í•œ content ë§Œë“¤ê¸°
                    if header_path:
                        content = header_path + " " + line
                    else:
                        content = line

                    texts['content'] = content
                    texts['start_index'] = line_match.start()
                    texts['end_index'] = line_match.end() - 1

                else: # ê¸°ì¡´ì— ìŒ“ì¸ textê°€ ìˆë‹¤ë©´ ê·¸ëƒ¥ í…ìŠ¤íŠ¸ë§Œ append
                    texts['content'] = texts['content'] + line
                    texts['end_index'] = line_match.end() - 1

        if texts: # ìŒ“ì¸ textsê°€ ìˆìœ¼ë©´ text_raw_chunksì— append
            text_raw_chunks.append({
                "type": "text",
                "content": texts['content'],
                "start_index": texts['start_index'],
                "end_index": texts['end_index']
            })

    else: # headerê°€ ì—†ëŠ” ë¬¸ì„œì¸ ê²½ìš° (Recursive Chunking)

        print("âš ï¸ No headers detected â€” applying RecursiveCharacterTextSplitter.")

        recursive_splitter = RecursiveCharacterTextSplitter(
            separators=[
                "\n\n",
                "\n",
                ". ",
                ", ",
                "! ",
                "? ",
                "; ",
                ": ",
                "- ",
                "â€¢ ",
                "\t",
                " "
            ],
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            is_separator_regex=False
        )

        recursive_split_mds = recursive_splitter.create_documents([md])
 
        for recursive_split_md in recursive_split_mds:

            text_pattern = re.escape(recursive_split_md.page_content)
            text_pattern = text_pattern.replace(r"\n", "\n")
            text_pattern = re.compile(text_pattern, re.DOTALL)

            text_match = text_pattern.search(md)

            if text_match:
                text_raw_chunks.append({
                    "type": "text",
                    "content": recursive_split_md.page_content,
                    "start_index": text_match.start(),
                    "end_index": text_match.end() - 1
                })
                
            else: # ì²­í¬ ë‚´ìš©ì´ ë§¤ì¹­ì´ ì•ˆë¼ë„ ì¼ë‹¨ ì¸ë±ìŠ¤ ì •ë³´ 0ìœ¼ë¡œ í•´ì„œ text raw chunk ë§Œë“¤ê³  append
                text_raw_chunks.append({
                    "type": "text",
                    "content": recursive_split_md.page_content,
                    "start_index": 0,
                    "end_index": 0
                })

    return text_raw_chunks

def attach_page_num_and_file_name(raw_chunks: list[dict], pages_info: list, file_name: str) -> list:
    """
    ê° ì²­í¬ì— page_numê³¼ file_nameì„ ìƒì„±í•˜ê³  start_index, end_index í‚¤ëŠ” ì‚­ì œí•©ë‹ˆë‹¤.

    Args:
        raw_chunks(list[dict]): ë¯¸ì™„ì„± chunk ë”•ì…”ë„ˆë¦¬ì˜ ë°°ì—´
        pages_info(list): get_pages_info() í•¨ìˆ˜ê°€ ë°˜í™˜í•œ ê° í˜ì´ì§€ì˜ ì¸ë±ìŠ¤ ë²”ìœ„ ì •ë³´
        file_name(str): ì‚¬ìš©ìë¡œë¶€í„° ë°›ì€ md íŒŒì¼ì˜ ì´ë¦„

    Returns:
        list: ì™„ì „í•œ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """

    for raw_chunk in raw_chunks:
        start_page = 0
        end_page = 0

        if raw_chunk['start_index'] == 0 and raw_chunk['end_index'] == 0:
            
            raw_chunk['page_num'] = [0]
            raw_chunk['file_name'] = file_name
            raw_chunk.pop('start_index', None)
            raw_chunk.pop('end_index', None)

            continue

        for page_num, (content_start_index, content_end_index) in pages_info:
            if content_start_index <= raw_chunk['start_index'] <= content_end_index:
                start_page = page_num

            if content_start_index <= raw_chunk['end_index'] <= content_end_index:
                end_page = page_num
                break

        raw_chunk['page_num'] = list(range(start_page, end_page + 1))
        raw_chunk['file_name'] = file_name
        raw_chunk.pop('start_index', None)
        raw_chunk.pop('end_index', None)

    return raw_chunks

def save_chunks_to_db(chunks: list[dict]) -> bool:
    """
    ì™„ì„±ëœ ì²­í¬ë“¤ì„ í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ Chunk DBì— ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        chunks(list): ì™„ì„±ëœ chunk ë”•ì…”ë„ˆë¦¬ì˜ ë°°ì—´

    Returns:
        bool: ì €ì¥ ì„±ê³µì—¬ë¶€
    """

    if not chunks:
        print("â„¹ï¸ No chunks to save. Skipping DB insert.")
        return True

    try:
        chunk_collection.insert_many(chunks)
        print(f"âœ… Saved {len(chunks)} chunks to DB.")
        return True

    except Exception as e:
        print(f"âŒ Failed to save chunks")
        traceback.print_exc()
        return False

def chunk_markdown_file(file_path: Path) -> bool:
    """
    ì£¼ì–´ì§„ markdown í…ìŠ¤íŠ¸ íŒŒì¼ì„ chunkingí•˜ê³ , ì™„ì„±ëœ ì²­í¬ë“¤ì„ Chunk DBì— ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        file_path(Path): ì²˜ë¦¬í•  markdown íŒŒì¼ ê²½ë¡œ

    Returns:
        bool: chunking í”„ë¡œì„¸ìŠ¤ ì„±ê³µì—¬ë¶€
            - True: chunking ë° DB ì €ì¥ ì„±ê³µ
            - False: chunking ê³¼ì • ë˜ëŠ” ì €ì¥ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ë°œìƒ
    """

    try:
        with file_path.open("r", encoding="utf-8") as f:
            md = f.read()

            pages_info = get_pages_info(md) # page ì¸ë±ìŠ¤ ë²”ìœ„ ì •ë³´ ì¶”ì¶œ

            md_without_page = remove_page(md) # >>> page ë§ˆì»¤ ì œê±°

            image_dict = generate_image_chunk(md_without_page) # image íƒ€ì… ì²˜ë¦¬
            md_without_image = image_dict['md_without_image']
            raw_chunks = image_dict['image_raw_chunks'].copy()

            link_dict = generate_link_chunk(md_without_image) # link íƒ€ì… ì²˜ë¦¬
            md_without_link = link_dict['md_without_link']
            raw_chunks = link_dict['link_raw_chunks'].copy()

            # table_dict = generate_table_chunk(md_without_link) # table íƒ€ì… ì²˜ë¦¬
            # md_without_table = table_dict['md_without_table']
            # raw_chunks.extend(table_dict['table_raw_chunks'])

            text_raw_chunks = generate_text_chunk(md_without_link) # text íƒ€ì… ì²˜ë¦¬
            raw_chunks.extend(text_raw_chunks)

            chunks = attach_page_num_and_file_name(raw_chunks, pages_info, file_path.stem) # raw chunksì— page_num, file_name ì¶”ê°€

            return save_chunks_to_db(chunks) # ì²­í¬ë“¤ DBì— ì €ì¥

    except Exception as e:
        print(f"ğŸ˜¢ Chunking failed for {file_path.name}: {e}")
        traceback.print_exc()
        return False

if __name__ == "__main__":

    # ìƒ˜í”Œë¡œ ì‚¬ìš©í•  markdown data ë¶ˆëŸ¬ì˜¤ê¸°
    # í˜„ì¬ëŠ” í”„ë¡œì íŠ¸ ë‚´ë¶€ì— ìˆëŠ” ìƒ˜í”Œ ë°ì´í„° í´ë”ì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.

    BASE_DIR = Path(__file__).resolve().parent # í˜„ì¬í´ë”ì˜ ê²½ë¡œ
    markdown_sample_data_folder_path = BASE_DIR / "markdown_sample_data" # ìƒ˜í”Œë¡œ ì“¸ markdown ë°ì´í„° í´ë”ì˜ ê²½ë¡œ

    for file_path in markdown_sample_data_folder_path.rglob("*.md"): # md íŒŒì¼ë§Œ ìˆœíšŒëŒê¸°

        is_chunking_succeeded = chunk_markdown_file(file_path)

        if is_chunking_succeeded:
            print(f"ğŸ‰ Chunking succeeded for {file_path.name}")
