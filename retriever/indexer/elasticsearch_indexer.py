from elasticsearch import Elasticsearch, helpers
from pymongo import MongoClient

from typing import List, Dict, Any, Iterable
import uuid
import os
from pathlib import Path
from dotenv import load_dotenv

# âœ… synonyms_path ëŠ” "ì—˜ë¼ìŠ¤í‹±ì„œì¹˜ ë…¸ë“œì˜ config ê¸°ì¤€ ê²½ë¡œ"ì—¬ì•¼ í•¨
# ì˜ˆ) $ES_CONFIG/synonyms/synonyms_ko_en.txt  -> ì—¬ê¸°ì„œ "synonyms/synonyms_ko_en.txt" ë¡œ ì§€ì •
SYN_PATH = "synonyms/synonyms_ko_en.txt"

class ElasticsearchIndexer:
    """
    MongoDBì˜ ì²­í‚¹ ë°ì´í„°ë¥¼ Elasticsearchì— ìƒ‰ì¸/ê²€ìƒ‰í•˜ëŠ” ìœ í‹¸.
    - í•œ/ì˜ ë¶„ì„ê¸° ë¶„ë¦¬ + ì˜ì–´ ê²€ìƒ‰ ì‹œ synonym_graph ì ìš©
    - fuzziness ì§€ì›
    - ë©€í‹° ì¸ë±ìŠ¤ ë™ì‹œ ê²€ìƒ‰
    - í•˜ì´ë¼ì´íŠ¸
    """

    def __init__(self):
        """
        MongoDB ë° Elasticsearch í´ë¼ì´ì–¸íŠ¸ì˜ ê°ì²´ë¥¼ ì–»ê³ , ì²­í‚¹ ë°ì´í„°ê°€ ì €ì¥ëœ MongoDB ì»¬ë ‰ì…˜ì„ ë³€ìˆ˜ì— í• ë‹¹í•©ë‹ˆë‹¤.
        """
        BASE_DIR = Path(__file__).resolve().parents[2]  # root ê²½ë¡œ
        load_dotenv(BASE_DIR / "common" / ".env")

        USERNAME = os.getenv("MONGO_INITDB_ROOT_USERNAME", "root")
        PASSWORD = os.getenv("MONGO_INITDB_ROOT_PASSWORD", "example")
        HOST = os.getenv("MONGO_HOST", "localhost")
        PORT = int(os.getenv("MONGO_PORT", 27017))

        self.mongodb_client = MongoClient(f"mongodb://{USERNAME}:{PASSWORD}@{HOST}:{PORT}/")
        self.chunk_collection = self.mongodb_client["chunk_db"]["chunk_collection"]

        ELASTIC_PASSWORD = os.getenv("ELASTIC_PASSWORD")
        self.elasticsearch_client = Elasticsearch(
            "http://localhost:9200",
            basic_auth=("elastic", ELASTIC_PASSWORD)
        )

    # --------------------------
    # 0) ì¸ë±ìŠ¤ ìƒì„± (ë§¤í•‘ + ë¶„ì„ê¸°)
    # --------------------------
    def ensure_index(self, index_name: str) -> None:
        es = self.elasticsearch_client
        if es.indices.exists(index=index_name):
            return

        body = {
            "settings": {
                # file_name ê°™ì€ keyword í•„ë“œì— ì†Œë¬¸ì ì •ê·œí™”ê°€ í•„ìš”í•˜ë©´ normalizer ì¶”ê°€ ê°€ëŠ¥
                # "analysis": { ... } ì•ˆì˜ "normalizer" ë¸”ë¡ì— ì •ì˜ í›„ í•„ë“œì— ì ìš©
                "analysis": {
                    "tokenizer": {
                        "edge_2_4": {"type": "edge_ngram", "min_gram": 2, "max_gram": 4}
                    },
                    "filter": {
                        # âœ… ê²€ìƒ‰(analyzer)ì—ì„œ ì‚¬ìš©í•  ë™ì˜ì–´. synonym_graphëŠ” search_analyzer ìª½ì—ë§Œ!
                        "syn_ko_en": {
                            "type": "synonym_graph",
                            "synonyms_path": SYN_PATH
                        },
                        "ko_pos_stop": {
                            "type": "nori_part_of_speech",
                            "stoptags": ["SP", "SSC", "SSO", "SC", "SE", "SF"]
                        }
                    },
                    "analyzer": {
                        # í•œêµ­ì–´: ì¸ë±ìŠ¤/ê²€ìƒ‰ ë™ì¼
                        "ko_index": {
                            "type": "custom",
                            "tokenizer": "nori_tokenizer",
                            "filter": ["ko_pos_stop"]
                        },
                        # ì˜ì–´(ì¸ë±ìŠ¤): ë™ì˜ì–´/ê·¸ë˜í”„ ì—†ì´ í‘œì¤€ í† í¬ë‚˜ì´ì§• + ì†Œë¬¸ì + ìŠ¤í…œ
                        "en_index": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": ["lowercase", "porter_stem"]
                        },
                        # ì˜ì–´(ê²€ìƒ‰): ë™ì˜ì–´ ê·¸ë˜í”„ ì ìš©
                        "en_search_with_syn": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": ["lowercase", "syn_ko_en", "porter_stem"]
                        },
                        # (ì„ íƒ) ì§§ì€ ì§ˆì˜/ìë™ì™„ì„± ë³´ì¡°ìš©
                        "ngram_ko": {
                            "tokenizer": "edge_2_4"
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "type": {"type": "keyword"},  # "text", "table", "image", "link" ë“±
                    "content": {
                        "type": "text",
                        "fields": {
                            "ko": {"type": "text", "analyzer": "ko_index"},
                            "en": {
                                "type": "text",
                                "analyzer": "en_index",
                                "search_analyzer": "en_search_with_syn"
                            },
                            # âš ï¸ synonym_graph ì™€ ngram ì€ ì„ì§€ ì•ŠëŠ” ê²Œ ì•ˆì •ì 
                            "ngram": {
                                "type": "text",
                                "analyzer": "ngram_ko",
                                "search_analyzer": "standard"
                            }
                        }
                    },
                    "metadata": {
                        "type": "text",
                        "fields": {
                            "ko": {"type": "text", "analyzer": "ko_index"},
                            "en": {
                                "type": "text",
                                "analyzer": "en_index",
                                "search_analyzer": "en_search_with_syn"
                            }
                        }
                    },
                    "file_info": {
                        "properties": {
                            "file_name": {"type": "keyword"},
                            "page_num": {"type": "integer"}  # âœ… ì •ìˆ˜ ë‹¨ì¼ê°’
                        }
                    }
                }
            }
        }

        es.indices.create(index=index_name, body=body)

    # --------------------------
    # 1) ë™ì˜ì–´ ì¬ì ìš© (í•«ë¦¬ë¡œë“œ)
    # --------------------------
    def reload_search_analyzers(self, index_name: str) -> Dict[str, Any]:
        """
        synonyms íŒŒì¼ì„ ê°±ì‹ í•œ ë’¤ ê²€ìƒ‰ ë¶„ì„ê¸°ë¥¼ ì¬ë¡œë“œ.
        ëª¨ë“  ES ë…¸ë“œì— ë™ì¼ ê²½ë¡œ/íŒŒì¼ì´ ë°°í¬ë˜ì–´ ìˆì–´ì•¼ í•¨.
        """
        return self.elasticsearch_client.indices.reload_search_analyzers(index=index_name)

    # --------------------------
    # 2-1) ìƒ‰ì¸(ë‹¨ì¼ ì¸ë±ìŠ¤)
    # --------------------------
    def index_file(self, file_name: str, index_name: str) -> bool:
        """
        íŠ¹ì • íŒŒì¼ì˜ ì²­í‚¹ ë°ì´í„°ë¥¼ Elasticsearch ì¸ë±ìŠ¤ì— ì¼ê´„ ìƒ‰ì¸.
        """
        self.ensure_index(index_name)

        cursor = self.chunk_collection.find({"file_info.file_name": file_name})
        first_chunk = next(cursor, None)
        if first_chunk is None:
            print(f"âš ï¸ No chunk data found for file: {file_name}")
            return False

        def _src(doc: Dict[str, Any]) -> Dict[str, Any]:
            fi = doc.get("file_info") or {}
            page_num = fi.get("page_num")
            # âœ… page_numì€ ì •ìˆ˜ë¡œ ë³´ì¥. ì—†ìœ¼ë©´ 0
            if isinstance(page_num, list):
                page_num = page_num[0] if page_num else 0
            elif page_num is None:
                page_num = 0

            return {
                "type": doc.get("type", ""),
                "content": doc.get("content") or "",
                "metadata": doc.get("metadata") or "",
                "file_info": {
                    "file_name": fi.get("file_name", ""),
                    "page_num": int(page_num)
                }
            }

        def generate_actions(first: Dict[str, Any], rest_cursor) -> Iterable[Dict[str, Any]]:
            yield {
                "_op_type": "index",
                "_index": index_name,
                "_id": str(first["_id"]),
                "_source": _src(first)
            }
            for doc in rest_cursor:
                yield {
                    "_op_type": "index",
                    "_index": index_name,
                    "_id": str(doc["_id"]),
                    "_source": _src(doc)
                }

        try:
            success_count, errors = helpers.bulk(
                self.elasticsearch_client,
                generate_actions(first_chunk, cursor),
                refresh="wait_for",
                raise_on_error=False
            )
        except Exception as e:
            print(f"âŒ Error indexing chunks: {e}")
            return False

        error_count = len(errors) if errors else 0
        print(f"âœ… Indexed {success_count} chunks into `{index_name}` with {error_count} errors.")
        if errors:
            print("\nâš ï¸ Detailed errors:")
            for i, err in enumerate(errors, start=1):
                print(f"  {i}. {err}\n")
        else:
            print("ğŸ‰ No errors during indexing!")
        return True

    # --------------------------
    # 2-2) ìƒ‰ì¸(ë©€í‹° ì¸ë±ìŠ¤)
    # --------------------------
    def index_chunks(self, chunks: list[dict], collections: list[str]) -> bool:
        """
        ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ì—¬ëŸ¬ Elasticsearch ì¸ë±ìŠ¤ì— ë™ì‹œ ìƒ‰ì¸.
        
        Args:
            chunks (list[dict]): ì €ì¥í•  ì²­í¬ ë¦¬ìŠ¤íŠ¸
            collections (list[str]): ì‚¬ìš©ìê°€ ì„ íƒí•œ collection ë¦¬ìŠ¤íŠ¸
        """

        if not chunks:
            print("âš ï¸ No chunks provided to index.")
            return False

        if not collections:
            print("âš ï¸ No index names provided.")
            return False

        # ì¸ë±ìŠ¤ë“¤ ì¡´ì¬ ì—¬ë¶€ ì²´í¬ ë° ìƒì„±
        for collection in collections:
            self.ensure_index(collection)

        def _src(doc: Dict[str, Any]) -> Dict[str, Any]:
            fi = doc.get("file_info") or {}
            page_num = fi.get("page_num")

            # page_numì„ intë¡œ í†µì¼
            if isinstance(page_num, list):
                page_num = page_num[0] if page_num else 0
            elif page_num is None:
                page_num = 0

            return {
                "type": doc.get("type", ""),
                "content": doc.get("content") or "",
                "metadata": doc.get("metadata") or "",
                "file_info": {
                    "file_uuid": fi.get("file_uuid", ""),
                    "file_name": fi.get("file_name", ""),
                    "collections": fi.get("collections", []),
                    "page_num": int(page_num)
                }
            }

        def generate_actions(target_index: str, chunks: list[dict]) -> Iterable[Dict[str, Any]]:
            for doc in chunks:
                yield {
                    "_op_type": "index",
                    "_index": target_index,
                    "_id": str(doc.get("file_info").get("file_uuid", uuid.uuid4())),
                    "_source": _src(doc)
                }

        overall_success = True

        # ì—¬ëŸ¬ ì¸ë±ìŠ¤ì— ê°ê° ìƒ‰ì¸ ì‹¤í–‰
        for collection in collections:
            print(f"\nğŸš€ Indexing {len(chunks)} chunks into index `{collection}` ...")

            try:
                success_count, errors = helpers.bulk(
                    self.elasticsearch_client,
                    generate_actions(collection, chunks),
                    refresh="wait_for",
                    raise_on_error=False
                )
            except Exception as e:
                print(f"âŒ Error indexing into `{collection}`: {e}")
                overall_success = False
                continue

            error_count = len(errors) if errors else 0
            print(f"âœ… Indexed {success_count} chunks into `{collection}` with {error_count} errors.")

            if errors:
                print("\nâš ï¸ Detailed errors:")
                for i, err in enumerate(errors, start=1):
                    print(f"  {i}. {err}\n")
                overall_success = False
            else:
                print("ğŸ‰ No errors during indexing!")

        return overall_success

    # --------------------------
    # 3) í‚¤ì›Œë“œ ê²€ìƒ‰
    # --------------------------
    def keyword_search(self, query: str, index_names: List[str]) -> List[Dict[str, Any]]:
        """
        í•œ/ì˜ + ë™ì˜ì–´(ì˜ì–´ ê²€ìƒ‰ ì‹œ) + ì˜¤íƒ€ í—ˆìš© + ë©€í‹° ì¸ë±ìŠ¤ ê²€ìƒ‰.
        - type in ["text","table"] -> content.*
        - type in ["image","link"]  -> metadata.*
        """
        index_expr = ",".join(index_names)
        RETURN_SIZE = 10
        fuzz = 1 if len(query) <= 3 else "AUTO"

        es_query = {
            "bool": {
                "should": [
                    {
                        "bool": {
                            "filter": [{"terms": {"type": ["text", "table"]}}],
                            "must": [{
                                "multi_match": {
                                    "query": query,
                                    "fields": [
                                        "content.ko^2.5",
                                        "content.en^2.5",
                                        "content.ngram^0.5"
                                    ],
                                    "type": "best_fields",
                                    "fuzziness": fuzz,
                                    "operator": "or"
                                }
                            }]
                        }
                    },
                    {
                        "bool": {
                            "filter": [{"terms": {"type": ["image", "link"]}}],
                            "must": [{
                                "multi_match": {
                                    "query": query,
                                    "fields": ["metadata.ko^1.5", "metadata.en^1.5"],
                                    "fuzziness": fuzz,
                                    "operator": "or"
                                }
                            }]
                        }
                    }
                ],
                "minimum_should_match": 1
            }
        }

        resp = self.elasticsearch_client.search(
            index=index_expr,
            size=RETURN_SIZE,
            query=es_query,
            track_total_hits=False,
            highlight={
                "fields": {
                    "content.ko": {},
                    "content.en": {},
                    "metadata.ko": {},
                    "metadata.en": {}
                }
            }
        )

        hits = resp.get("hits", {}).get("hits", [])
        results: List[Dict[str, Any]] = []
        for h in hits:
            src = h.get("_source", {})
            results.append({
                "score": h.get("_score", 0.0),
                "type": src.get("type"),
                "content": src.get("content"),
                "metadata": src.get("metadata"),
                "file_info": src.get("file_info", {}),
                "highlight": h.get("highlight", {})
            })

        print(f"âœ… Found {len(results)} results")
        for i, r in enumerate(results[:RETURN_SIZE], 1):
            fn = (r.get("file_info") or {}).get("file_name")
            pg = (r.get("file_info") or {}).get("page_num")
            print(f"--- Result {i} (score: {r['score']:.4f}) ---")
            print(f"Type: {r.get('type')} | File: {fn} | Page: {pg}")
            hl = r.get("highlight") or {}
            snippet_list = (
                hl.get("content.en")
                or hl.get("content.ko")
                or hl.get("metadata.en")
                or hl.get("metadata.ko")
                or [ (r.get("content") or r.get("metadata") or "")[:200] ]
            )
            print(f"Snippet: {snippet_list[0]}\n")
        return results


def main():
    """
    ElasticsearchIndexerë¥¼ í†µí•´ í‚¤ì›Œë“œ ê²€ìƒ‰ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.
    ë¨¼ì € í…ŒìŠ¤íŠ¸í•˜ê³  ì‹¶ì€ md ë¬¸ì„œì˜ ì²­í‚¹ì„ ì™„ë£Œí•œ í›„ì— ì‹¤í–‰í•´ì£¼ì„¸ìš”.
    """
    indexer = ElasticSearchIndexer()

    # MongoDBì— ì €ì¥ëœ íŒŒì¼ ëª©ë¡ í™•ì¸
    available_files = indexer.chunk_collection.distinct("file_info.file_name")
    print(f"[INFO] Available files in MongoDB ({len(available_files)} files):")
    for i, file in enumerate(available_files[:10], 1):  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
        print(f"  {i}. {file}")
    if len(available_files) > 10:
        print(f"  ... and {len(available_files) - 10} more files")
    print()

    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” íŒŒì¼ëª…ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    if available_files:
        test_file = available_files[0]
        print(f"[TEST] Testing with file: {test_file}\n")

        # ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ (ë™ì˜ì–´ ì„¤ì • ì ìš© ìœ„í•´)
        if indexer.elasticsearch_client.indices.exists(index="msds"):
            print("[DELETE] Deleting existing 'msds' index to apply new synonym settings...")
            indexer.elasticsearch_client.indices.delete(index="msds")

        # ìƒˆë¡œ ìƒ‰ì¸
        indexer.index_file(test_file, "msds")

        # í•œê¸€ë¡œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸!
        print("\n" + "="*50)
        print("[SEARCH] Test 1: Search with Korean 'ì¹´ìŠ¤ë²ˆí˜¸'")
        print("="*50)
        indexer.keyword_search("ì¹´ìŠ¤ë²ˆí˜¸", ["msds"])

        print("\n" + "="*50)
        print("[SEARCH] Test 2: Search with Korean 'ë“ëŠ”ì '")
        print("="*50)
        indexer.keyword_search("ë“ëŠ”ì ", ["msds"])
    else:
        print("âŒ No files found in MongoDB. Please run chunking first.")


if __name__ == "__main__":
    main()
