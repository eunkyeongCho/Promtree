from typing import List, Dict, Any
from elasticsearch import helpers

from db.mongodb import get_mongodb_client
from db.elasticsearch.elasticsearch import get_elasticsearch_client


class ElasticSearchIndexer:
    """
    MongoDBì—ì„œ ìƒì„±ëœ ì²­í‚¹(chunk) ë°ì´í„°ë¥¼ Elasticsearchì— ìƒ‰ì¸(indexing)í•˜ê³ , ì €ì¥ëœ ë°ì´í„°ë¥¼ ê²€ìƒ‰(query)í•  ìˆ˜ ìˆëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    """
    def __init__(self):
        """
        MongoDB ë° Elasticsearch í´ë¼ì´ì–¸íŠ¸ì˜ ì‹±ê¸€í†¤ ê°ì²´ë¥¼ ì–»ê³ , ì²­í‚¹ ë°ì´í„°ê°€ ì €ì¥ëœ MongoDB ì»¬ë ‰ì…˜ì„ ë³€ìˆ˜ì— í• ë‹¹í•©ë‹ˆë‹¤.
        """
        self.mongodb_client = get_mongodb_client()
        self.elasticsearch_client = get_elasticsearch_client()
        self.chunk_collection = self.mongodb_client["chunk_db"]["chunk_collection"]


    def index_file(self, file_name: str, index_name: str) -> bool:
        """
        MongoDBì— ì €ì¥ëœ íŠ¹ì • íŒŒì¼ì˜ ì²­í‚¹(chunk) ë°ì´í„°ë¥¼ Elasticsearch ì¸ë±ìŠ¤ì— ì¼ê´„ ìƒ‰ì¸í•©ë‹ˆë‹¤.

        Args:
            file_name (str):
                ìƒ‰ì¸ ëŒ€ìƒ ì›ë³¸ íŒŒì¼ ì´ë¦„
            index_name (str):
                ìƒ‰ì¸ì´ ì €ì¥ë  Elasticsearch ì¸ë±ìŠ¤ ì´ë¦„(msds, tds ë‘˜ ì¤‘ í•˜ë‚˜)
                í™”ë©´ì—ì„œë¶€í„° ì‚¬ìš©ìê°€ PDFë¥¼ ì—…ë¡œë“œí•  MSDS/TDS ì„ íƒí•˜ê¸°ë¡œ í–ˆìœ¼ë¯€ë¡œ index_nameì„ ë„˜ê²¨ì¤„ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ íŒë‹¨í•¨.

        Returns:
            bool:
                - ìƒ‰ì¸ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜í–‰ë˜ë©´ True
                - íŒŒì¼ì— ëŒ€ì‘í•˜ëŠ” ì²­í‚¹ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ìƒ‰ì¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ False
        """

        # MongoDBì—ì„œ ì²­í¬ë“¤ ê°€ì ¸ì˜¤ê¸°
        chunks = self.chunk_collection.find({"file_info.file_name": file_name})

        # ì²­í¬ ë°ì´í„°ë¥¼ ëª¨ë‘ ë©”ëª¨ë¦¬ì— ì˜¬ë¦¬ë©´ ë¹„íš¨ìœ¨ì ì´ë¯€ë¡œ, ì²«ë²ˆì§¸ ì²­í¬ ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì²­í¬ ì¡´ì¬ì—¬ë¶€ í™•ì¸
        first_chunk = next(chunks, None)

        if first_chunk is None:
            print(f"âš ï¸ No chunk data found for file: {file_name}")
            return False

        # ì²­í¬ê°€ ì¡´ì¬í•œë‹¤ë©´ ì¸ë±ì‹± ì§„í–‰
        # actionì€ ê³µì‹ë¬¸ì„œì˜ í‘œí˜„ì´ì–´ì„œ ë”°ë¦„
        # ì¸ë±ì‹±í•  ë°ì´í„°ë¥¼ ëª¨ë‘ ë©”ëª¨ë¦¬ì— ì˜¬ë¦¬ì§€ ì•Šê³ , generatorë¥¼ í†µí•´ ë°ì´í„°ë¥¼ í•˜ë‚˜ì”© í˜ë ¤ë³´ë‚´ëŠ” ê²ƒì´ Elasticsearch ê³µì‹ë¬¸ì„œì—ì„œ ê¶Œì¥í•˜ëŠ” ë°©ì‹ì´ë¯€ë¡œ ë”°ë¦„
        def generate_actions():
            # ì²« ë¬¸ì„œë¶€í„° ì²˜ë¦¬
            yield {
                "_op_type": "index",
                "_index": index_name,
                "_id": str(first_chunk["_id"]), # MongoDBì˜ _id ê°’ì„ ê·¸ëŒ€ë¡œ Elasticsearchì˜ _id ê°’ìœ¼ë¡œ ì‚¬ìš©
                "_source": {
                    "type": first_chunk.get("type", ""),
                    "content": first_chunk.get("content", ""),
                    "metadata": first_chunk.get("metadata", ""),
                    "file_info": first_chunk.get("file_info", {})
                }
            }

            # ë‚˜ë¨¸ì§€ ë¬¸ì„œ ì²˜ë¦¬
            for chunk in chunks:
                yield {
                    "_op_type": "index",
                    "_index": index_name,
                    "_id": str(chunk["_id"]),
                    "_source": {
                        "type": chunk.get("type", ""),
                        "content": chunk.get("content", ""),
                        "metadata": chunk.get("metadata", ""),
                        "file_info": chunk.get("file_info", {})
                    }
                }

        try:
            (success_count, errors) = helpers.bulk(self.elasticsearch_client, generate_actions())

        except Exception as e:
            print(f"âŒ Error indexing chunks: {e}")
            return False

        error_count = len(errors) if errors else 0

        print(f"âœ… Indexed {success_count} chunks into `{index_name}` with {error_count} errors.")
        
        if errors and error_count > 0:
            print("\nâš ï¸ Detailed errors:")
            for i, err in enumerate(errors, start=1):
                print(f"  {i}. {err}\n")
        else:
            print("ğŸ‰ No errors during indexing!")

        return True


    def keyword_search(self, query: str, index_names: list[str]) -> List[Dict[str, Any]]:
        """
        Elasticsearchì—ì„œ ê²€ìƒ‰ì–´(query)ì— ë”°ë¼ ì²­í¬ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        ì²­í¬ì˜ type ê°’ì— ë”°ë¼ ê²€ìƒ‰ ê¸°ì¤€ í•„ë“œê°€ ë‹¬ë¼ì§‘ë‹ˆë‹¤.
            - type = text or table â†’ content í•„ë“œì—ì„œ ê²€ìƒ‰
            - type = image or link â†’ metadata í•„ë“œì—ì„œ ê²€ìƒ‰
        
        index_namesê°€ ì—¬ëŸ¬ê°œì¸ ê²½ìš°, ëª¨ë“  ì¸ë±ìŠ¤ì— ëŒ€í•´ RETURN_SIZEë§Œí¼ì˜ ì²­í¬ë¥¼ ê²€ìƒ‰í•œ í›„ ìƒìœ„ RETURN_SIZEê°œì˜ ì²­í¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            query (str): ì‚¬ìš©ìì˜ query
            index_names (list[str]): ê²€ìƒ‰ì„ ìˆ˜í–‰í•  Elasticsearch ì¸ë±ìŠ¤ ëª©ë¡ (ê²€ìƒ‰ì°½ì—ì„œ @ë¡œ íƒœê·¸í•˜ëŠ” ê²ƒ)

        Returns:
            List[Dict[str, Any]]:
                ê²€ìƒ‰ëœ ë¬¸ì„œ ëª©ë¡. ê° ë¬¸ì„œëŠ” ë‹¤ìŒ êµ¬ì¡°ë¥¼ ê°€ì§„ë‹¤:
                {
                    "type": str,
                    "content": str | None,
                    "metadata": str | None,
                    "file_info": {
                        "file_name": str,
                        "page_num": list[int]
                    }
                }
        """

        elasticsearch_query = {
            "bool": {
                "should": [
                    {
                        "bool": {
                            "must": [
                                {"match": {"content": query}}
                            ],
                            "filter": [
                                {"terms": {"type": ["text", "table"]}}
                            ]
                        }
                    },
                    {
                        "bool": {
                            "must": [
                                {"match": {"metadata": query}}
                            ],
                            "filter": [
                                {"terms": {"type": ["image", "link"]}}
                            ]
                        }
                    }
                ],
                "minimum_should_match": 1
            }
        }

        RETURN_SIZE = 10 # ë°˜í™˜í•  ì²­í¬ ìˆ˜
        all_hits = []

        for index_name in index_names:
            response = self.elasticsearch_client.search(
                index=index_name,
                size=RETURN_SIZE,
                query=elasticsearch_query
            )
            all_hits.extend(response["hits"]["hits"])

        scored_results = sorted(
            [
                {
                    "score": hit["_score"],
                    "type": hit["_source"].get("type"),
                    "content": hit["_source"].get("content"),
                    "metadata": hit["_source"].get("metadata"),
                    "file_info": hit["_source"].get("file_info")
                }
                for hit in all_hits
            ],
            key=lambda result: result["score"],
            reverse=True
        )

        scored_results = scored_results[:RETURN_SIZE]

        results = [
            {
                "type": scored_result["type"],
                "content": scored_result["content"],
                "metadata": scored_result["metadata"],
                "file_info": scored_result["file_info"]
            }
            for scored_result in scored_results
        ]

        print(f"âœ… Found {len(results)} results")

        for i, r in enumerate(results, start=1):
            print(f"--- Result {i} (score: {scored_results[i-1]['score']:.4f}) ---")
            print(f"Type: {r['type']}")
            print(f"File: {r['file_info'].get('file_name')} | Page: {r['file_info'].get('page_num')}")
            if r['type'] in ["text", "table"]:
                print(f"Content: {r['content'][:200]}...")  # ê¸¸ë©´ ì• 200ìë§Œ ì¶œë ¥
            else:
                print(f"Metadata: {r['metadata']}")
            print()

        return results


def main():
    """
    ElasticsearchIndexerë¥¼ í†µí•´ í‚¤ì›Œë“œ ê²€ìƒ‰ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.
    ë¨¼ì € í…ŒìŠ¤íŠ¸í•˜ê³  ì‹¶ì€ md ë¬¸ì„œì˜ ì²­í‚¹ì„ ì™„ë£Œí•œ í›„ì— ì‹¤í–‰í•´ì£¼ì„¸ìš”.
    """
    indexer = ElasticSearchIndexer()

    indexer.index_file("000000002914_AU_EN", "msds")
    indexer.keyword_search("Triethylene Glycolì˜ casë²ˆí˜¸", ["msds"])


if __name__ == "__main__":
    main()
