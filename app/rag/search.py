from elasticsearch import Elasticsearch
from qdrant_client import QdrantClient
from sentence_transformers import SentenceTransformer
from qdrant_client.models import VectorParams, Distance
from neo4j import GraphDatabase

import os
from dotenv import load_dotenv
import json
import requests
import asyncio
import httpx
from openai import OpenAI

from app.rag.elasticsearch_indexer import ElasticsearchIndexer
from app.rag.retriever import query_embedding, search_similar_chunks
from app.rag.neo4j_knowledge_graph import Neo4jKnowledgeGraph


class Search:
    load_dotenv()

    def __init__(self):

        # --- Indexing Database Client ---
        ELASTIC_PASSWORD = os.getenv("ELASTIC_PASSWORD")

        self.indexing_db_client = Elasticsearch(
            "http://localhost:9200",
            basic_auth=("elastic", ELASTIC_PASSWORD)
        )

        # --- Embedding Model Client ---
        self.embedding_model = SentenceTransformer('Qwen/Qwen3-Embedding-0.6B', trust_remote_code=True)

        # --- Vector Database Client ---
        self.vector_db_client = QdrantClient(url="http://localhost:6333")

        # collections = ["msds", "tds"]
        # for collection in collections:
        #     self.vector_db_client.recreate_collection(
        #         collection_name=collection,
        #         vectors_config=VectorParams(
        #             size=1024,
        #             distance=Distance.COSINE
        #         )
        #     )
        #     print(f"ì»¬ë ‰ì…˜ '{collection}' ìƒì„± ì™„ë£Œ.")

        # --- Knowledge Graph Client ---
        NEO4J_URI = os.getenv("NEO4J_URI")
        NEO4J_AUTH = ("neo4j", os.getenv("NEO4J_PASSWORD"))

        self.neo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)


    def _keyword_search(self, query: str, collections: list[str]):
        """
        í‚¤ì›Œë“œ ê²€ìƒ‰ ìˆ˜í–‰
        """
        elasticsearch_indexer = ElasticsearchIndexer()
        elasticsearch_indexer.ensure_index(collections)
        return elasticsearch_indexer.keyword_search(query, collections)

    def _vector_search(self, query: str, collections: list[str]):
        """
        ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        """
        qv = query_embedding(self.embedding_model, query)
        print("ì¿¼ë¦¬ ì„ë² ë”© ì™„ë£Œ")
        results = search_similar_chunks(self.vector_db_client, qv, collections, 5)
        print("ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ")
        return results

    def postprecessing(self, chunks: list[dict], type: str) -> list[dict]:
        """
        ì²­í¬ í›„ì²˜ë¦¬
        """
        if type == "keyword":
            normalized = []
            for es_result in chunks:
                chunk_type = es_result.get("type", "")
                file_info = es_result.get("file_info", {})
                
                # image íƒ€ì…ì¸ ê²½ìš° metadataë¥¼, ê·¸ ì™¸ì—ëŠ” contentë¥¼ ì‚¬ìš©
                if chunk_type == "image":
                    content = es_result.get("metadata", "")
                else:
                    content = es_result.get("content", "")
                
                normalized.append({
                    "content": content,
                    "documentId": file_info.get("file_uuid", ""),
                    "file_name": file_info.get("file_name", ""),
                    "page_nums": file_info.get("page_num", []),
                    "snippet": content[:200] + "...",
                })
            return normalized
        elif type == "vector":
            normalized = []
            for qdrant_result in chunks:
                chunk = qdrant_result.get("chunk", {})
                chunk_type = chunk.get("type", "")
                file_info = chunk.get("file_info", {})
                
                # image íƒ€ì…ì¸ ê²½ìš° metadataë¥¼, ê·¸ ì™¸ì—ëŠ” contentë¥¼ ì‚¬ìš©
                if chunk_type == "image":
                    content = chunk.get("metadata", "")
                else:
                    content = chunk.get("content", "")
                
                normalized.append({
                    "content": content,
                    "documentId": file_info.get("file_uuid", ""),
                    "file_name": file_info.get("file_name", ""),
                    "page_nums": file_info.get("page_num", []),
                    "snippet": content[:200] + "...",
                })
            return normalized
        elif type == "graph":
            normalized = []
            for graph_result in chunks:
                graph_content = graph_result.get("graph", "")
                file_info = graph_result.get("file_info", {})
                
                normalized.append({
                    "content": graph_content,
                    "documentId": file_info.get("file_uuid", ""),
                    "file_name": file_info.get("file_name", ""),
                    "page_nums": file_info.get("page_num", []),
                    "snippet": graph_content[:200] + "...",
                })
            return normalized
    
    
    def extract_sources(self, chunks: list[dict]) -> list[dict]:
       """
       í›„ì²˜ë¦¬ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ì„œë³„ë¡œ ê·¸ë£¹í•‘í•˜ì—¬ sources êµ¬ì¡°ë¡œ ë³€í™˜
       """
       grouped = {}
       for entry in chunks:
           doc_id = entry.get("documentId", "")
           file_name = entry.get("file_name", "")
           page_nums = entry.get("page_nums") or [0]
           
           if not doc_id:
               # documentIdê°€ ì—†ìœ¼ë©´ viewer URLì„ ë§Œë“¤ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.
               # (ì´ ê²½ìš°ì—ë„ snippet/textëŠ” rankingì—ëŠ” ì‚¬ìš©ë©ë‹ˆë‹¤.)
               url = None
           else:
               url = f"/{doc_id}/view"

           group = grouped.setdefault(doc_id, {
               "title": file_name,
               "documentId": doc_id,
               "url": url,
               "chunks": []
           })
           
           group["chunks"].append({
               "pageRange": {"start": page_nums[0], "end": page_nums[-1]},
               "snippet": entry.get("snippet", ""),
               "text": entry.get("content", ""),
           })
       return list(grouped.values())


    async def _async_graph_search(self, query: str):
        """
        ê·¸ë˜í”„ ê²€ìƒ‰ ìˆ˜í–‰
        """
        neo4j_knowledge_graph = Neo4jKnowledgeGraph()
        return await neo4j_knowledge_graph.async_search_graph(query)

    async def async_generate_rag_answer(self, query: str, collections: list[str], history: list[dict] | None = None):
        """
        ë‹µë³€ ìƒì„± ìˆ˜í–‰
        """
        print(f"\n{'='*80}")
        print(f"ğŸ” [RAG] async_generate_rag_answer ì‹œì‘")
        print(f"   Query: {query}")
        print(f"   Collections: {collections}")
        print(f"   History length: {len(history) if history else 0}")
        print(f"{'='*80}\n")
        
        try:
            print(f"ğŸ“Š [RAG] í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œì‘...")
            keyword_raw = self._keyword_search(query, collections)
            print(f"   âœ… í‚¤ì›Œë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(keyword_raw)}ê°œ ê²°ê³¼")
            keyword_chunks = self.postprecessing(keyword_raw, "keyword")
            print(f"   âœ… í‚¤ì›Œë“œ í›„ì²˜ë¦¬ ì™„ë£Œ: {len(keyword_chunks)}ê°œ ì²­í¬\n")
        except Exception as e:
            print(f"   âŒ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            raise
        
        try:
            print(f"ğŸ”¢ [RAG] ë²¡í„° ê²€ìƒ‰ ì‹œì‘...")
            vector_raw = self._vector_search(query, collections)
            print(f"   âœ… ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {len(vector_raw)}ê°œ ê²°ê³¼")
            vector_chunks = self.postprecessing(vector_raw, "vector")
            print(f"   âœ… ë²¡í„° í›„ì²˜ë¦¬ ì™„ë£Œ: {len(vector_chunks)}ê°œ ì²­í¬\n")
        except Exception as e:
            print(f"   âŒ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            raise
        
        try:
            print(f"ğŸ•¸ï¸  [RAG] ê·¸ë˜í”„ ê²€ìƒ‰ ì‹œì‘...")
            graph_raw = await self._async_graph_search(query)
            print(f"   âœ… ê·¸ë˜í”„ ê²€ìƒ‰ ì™„ë£Œ: {len(graph_raw)}ê°œ ê²°ê³¼")
            graph_chunks = self.postprecessing(graph_raw, "graph")
            print(f"   âœ… ê·¸ë˜í”„ í›„ì²˜ë¦¬ ì™„ë£Œ: {len(graph_chunks)}ê°œ ì²­í¬\n")
        except Exception as e:
            print(f"   âŒ ê·¸ë˜í”„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            raise

        keyword_results = json.dumps(keyword_chunks, ensure_ascii=False, indent=2)
        vector_results = json.dumps(vector_chunks, ensure_ascii=False, indent=2)
        graph_results = json.dumps(graph_chunks, ensure_ascii=False, indent=2)

        ranked_chunks = keyword_chunks + vector_chunks + graph_chunks
        print(f"ğŸ“¦ [RAG] ì „ì²´ ì²­í¬ í†µí•©: {len(ranked_chunks)}ê°œ (í‚¤ì›Œë“œ: {len(keyword_chunks)}, ë²¡í„°: {len(vector_chunks)}, ê·¸ë˜í”„: {len(graph_chunks)})")
        
        sources = self.extract_sources(ranked_chunks)
        print(f"ğŸ“š [RAG] Sources ì¶”ì¶œ ì™„ë£Œ: {len(sources)}ê°œ ë¬¸ì„œ\n")

        prompt = f"""
        ë‹¹ì‹ ì€ ì‚¼ì„±ì „ì ìƒì‚°ê¸°ìˆ ì—°êµ¬ì†Œì˜ ì†Œì¬ ë¬¼ì„± ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ê·¼ê±° ì¤‘ì‹¬ì˜ ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ë‹¹ì‹ ì˜ ëª¨ë“  ë‹µë³€ì€ ì•„ë˜ ì œê³µëœ ë¬¸ì„œ(JSON í˜•íƒœ)ì˜ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ í•´ì•¼ í•©ë‹ˆë‹¤. 
        ì¶”ë¡ ì„ í•  ë•Œë„ ë°˜ë“œì‹œ ë¬¸ì„œì˜ ë‚´ìš©ì„ ê·¼ê±°ë¡œ í•´ì•¼ í•˜ë©°, ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ì¸¡í•˜ì§€ ë§ê³  ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”.

        -----------------------------
        [ì‚¬ìš©ì ì§ˆë¬¸]
        {query}
        -----------------------------

        [ê²€ìƒ‰ëœ ë¬¸ì„œ(JSON)]
        ì•„ë˜ JSON ë°°ì—´ì˜ ê° ìš”ì†ŒëŠ” ê²€ìƒ‰ëœ ë¬¸ì„œ ì¡°ê°(chunk)ì…ë‹ˆë‹¤.
        ê° chunkëŠ” ë‹¤ìŒ ê°’ì„ í¬í•¨í•©ë‹ˆë‹¤:
        - type: text/table/image/link ë“± ë¬¸ì„œ ìœ í˜•
        - content: imgaeì˜ ê²½ë¡œ | linkì˜ ì›ë³¸ë§í¬ | List[dict[str, str]] í˜•íƒœë¡œ ì–¸í”¼ë´‡ëœ html tableì˜ ë‚´ìš© | textì˜ ë‚´ìš©
        - metadata: imageì˜ ë©”íƒ€ë°ì´í„° | linkì˜ ë©”íƒ€ë°ì´í„° | None
        - file_info: {{
            "file_uuid" : "ë°±ì—”ë“œì—ì„œ ë„˜ì–´ì˜¤ëŠ” Doc ID",
            "file_name" : "íŒŒì¼ ì´ë¦„",
            "collections" : ["collection ì´ë¦„1", "collection ì´ë¦„2", ...],
            "page_num" : í˜ì´ì§€ ë²ˆí˜¸ ì •ìˆ˜ ë°°ì—´
        }}

        ê²€ìƒ‰ëœ ë¬¸ì„œ(JSON):
        [í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼]
        {keyword_results}

        [ë²¡í„° ê²€ìƒ‰ ê²°ê³¼]
        {vector_results}

        [ê·¸ë˜í”„ ê²€ìƒ‰ ê²°ê³¼]
        {graph_results}

        -----------------------------
        [ì§€ì¹¨]

        1. ë°˜ë“œì‹œ ë¬¸ì„œ(JSON) ì† text ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
        2. ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
        3. ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ì°¸ì¡°í–ˆë‹¤ë©´ ë‹µë³€ì— ê·¸ ë‚´ìš©ì„ ëª¨ë‘ ë°˜ì˜í•˜ì„¸ìš”.
        4. ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” "ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ì„¸ìš”.
        5. JSON ì•ˆì˜ êµ¬ì¡°(key ì´ë¦„)ëŠ” ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
        6. image / link íƒ€ì… chunkëŠ” metadataë¥¼ ìš”ì•½í•´ í…ìŠ¤íŠ¸ì²˜ëŸ¼ ë‹¤ë¤„ë„ ë©ë‹ˆë‹¤.

        -----------------------------
        [ì¶œë ¥ í˜•ì‹]
        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:

        {{
            "answer" : "ë‹µë³€ ë‚´ìš©"
        }}
        """

        # Upstage API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        upstage_key = os.getenv("UPSTAGE_API_KEY")
        print(f"ğŸ”‘ [RAG] Upstage API Key í™•ì¸: {'âœ… ì„¤ì •ë¨' if upstage_key else 'âŒ ì—†ìŒ'}")
        
        if not upstage_key:
            raise ValueError("UPSTAGE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        client = OpenAI(
            api_key=upstage_key,
            base_url="https://api.upstage.ai/v1",
            http_client=httpx.Client()
        )
        print(f"ğŸ¤– [RAG] Upstage API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ\n")

        # ì´ì „ ëŒ€í™” ë§¥ë½ì„ messages í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        messages = []
        if history:
            for msg in history:
                role = msg.get("role", "user")
                contents = msg.get("contents", "")
                # roleì´ "chatbot"ì´ë©´ "assistant"ë¡œ ë³€í™˜
                if role == "chatbot":
                    role = "assistant"
                elif role not in ["user", "assistant", "system"]:
                    role = "user"
                messages.append({
                    "role": role,
                    "content": contents
                })
        
        # í˜„ì¬ ì§ˆë¬¸ê³¼ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
        messages.append({
            "role": "user",
            "content": prompt
        })

        # ë‹µë³€ ìš”ì²­
        print(f"ğŸš€ [RAG] Upstage API í˜¸ì¶œ ì‹œì‘...")
        print(f"   Model: solar-pro")
        print(f"   Messages count: {len(messages)}")
        print(f"   Prompt length: {len(prompt)} characters\n")
        
        try:
            response = client.chat.completions.create(
                model="solar-pro",
                messages=messages,
                temperature=0.7,
                stream=False
            )
            
            print(f"âœ… [RAG] Upstage API ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
            llm_answer_raw = response.choices[0].message.content
            print(f"   ì‘ë‹µ ê¸¸ì´: {len(llm_answer_raw)} characters")
            print(f"ğŸ” LLM response: {llm_answer_raw[:200]}..." if len(llm_answer_raw) > 200 else f"ğŸ” LLM response: {llm_answer_raw}\n")
            
            try:
                answer_payload = json.loads(llm_answer_raw)
                answer = answer_payload.get("answer", "")
                print(f"âœ… [RAG] JSON íŒŒì‹± ì„±ê³µ: answer ê¸¸ì´ {len(answer)} characters\n")
            except json.JSONDecodeError as e:
                print(f"âš ï¸  [RAG] JSON íŒŒì‹± ì‹¤íŒ¨, ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©: {e}")
                answer = llm_answer_raw
                
        except Exception as e:
            print(f"\nâŒ [RAG] Upstage API request failed:")
            print(f"   Error type: {type(e).__name__}")
            print(f"   Error message: {str(e)}")
            if hasattr(e, 'response'):
                print(f"   Response status: {e.response.status_code if hasattr(e.response, 'status_code') else 'N/A'}")
            print(f"{'='*80}\n")
            raise

        print(f"âœ… [RAG] async_generate_rag_answer ì™„ë£Œ")
        print(f"   Answer: {answer[:100]}..." if len(answer) > 100 else f"   Answer: {answer}")
        print(f"   Sources: {len(sources)}ê°œ\n")
        print(f"{'='*80}\n")
        
        return (answer, sources)


if __name__ == "__main__":
    search = Search()

    questions = [
        "í†¨ë£¨ì—”ì˜ ë“ëŠ”ì  ë²”ìœ„ëŠ”?",
        "ì•„ì„¸í‹¸ë Œì˜ CAS ë²ˆí˜¸ëŠ” ë¬´ì—‡ì¸ê°€?",
        "ìˆ˜ì†Œì¶©ì „ì†Œìš© ìˆ˜ì†Œì˜ ê¶Œê³ ìš©ë„ëŠ”?"
    ]

    for question in questions:
        print(f"\n{'='*80}")
        print(f"ì§ˆë¬¸: {question}")
        print(f"{'='*80}\n")
        asyncio.run(search.async_generate_rag_answer(question, ["msds"]))