from elasticsearch import Elasticsearch
from qdrant_client import QdrantClient
from qdrant_client.models import Filter, FieldCondition, MatchValue, VectorParams, Distance
from sentence_transformers import SentenceTransformer
from neo4j import GraphDatabase
from pymongo import MongoClient

import os
from dotenv import load_dotenv
import asyncio

from app.rag.markdown_chunker import MarkdownChunker
from app.rag.embedding import chunk_embedding_and_upsert
from app.rag.elasticsearch_indexer import ElasticsearchIndexer
from app.rag.neo4j_knowledge_graph import Neo4jKnowledgeGraph


class PdfIngestionPipeline:
    load_dotenv()

    def __init__(self):
        # --- Markdown Folder Path ---
        self.markdown_path = "markdown_sample_data"

        # --- Markdown Chunker ---
        self.markdown_chunker = MarkdownChunker()

        # --- Indexing Database Client ---
        ELASTIC_PASSWORD = os.getenv("ELASTIC_PASSWORD")

        self.indexing_db_client = Elasticsearch(
            "http://localhost:9200",
            basic_auth=("elastic", ELASTIC_PASSWORD)
        )

        self.elasticsearch_indexer = ElasticsearchIndexer()

        # --- Embedding Model Client ---
        self.embedding_model = SentenceTransformer('Qwen/Qwen3-Embedding-0.6B', trust_remote_code=True)

        # --- Vector Database Client ---
        self.vector_db_client = QdrantClient(url="http://localhost:6333")
        

        # --- MongoDB Client (collection name ë™ê¸°í™”) ---
        mongo_url = (
            f"mongodb://{os.getenv('MONGO_INITDB_ROOT_USERNAME')}:{os.getenv('MONGO_INITDB_ROOT_PASSWORD')}"
            f"@{os.getenv('MONGO_HOST')}:{os.getenv('MONGO_PORT')}"
        )
        self.mongo_client = MongoClient(mongo_url)
        self.mongo_db = self.mongo_client["promtree"]
        self.collections_collection = self.mongo_db["collections"]
        collections = self._fetch_collection_names()

        for collection in collections:
            if not self.vector_db_client.collection_exists(collection):
                self.vector_db_client.create_collection(
                    collection_name=collection,
                    vectors_config=VectorParams(
                        size=1024,
                        distance=Distance.COSINE
                    )
                )
                print(f"ì»¬ë ‰ì…˜ '{collection}' ìƒì„± ì™„ë£Œ.")
            else:
                print(f"ì»¬ë ‰ì…˜ '{collection}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ê¸°ì¡´ ì»¬ë ‰ì…˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

        # --- Knowledge Graph Client ---
        NEO4J_URI = os.getenv("NEO4J_URI")
        NEO4J_AUTH = ("neo4j", os.getenv("NEO4J_PASSWORD"))

        self.neo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)

    def _fetch_collection_names(self) -> list[str]:
        """
        MongoDBì—ì„œ KnowledgeCollection ì´ë¦„ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        """
        try:
            cursor = self.collections_collection.find({}, {"name": 1, "_id": 0})
            names = sorted({doc["name"] for doc in cursor if doc.get("name")})
            if not names:
                print("âš ï¸ MongoDBì— ë“±ë¡ëœ collectionì´ ì—†ìŠµë‹ˆë‹¤. Qdrant ì»¬ë ‰ì…˜ ìƒì„± ë‹¨ê³„ëŠ” ê±´ë„ˆëœë‹ˆë‹¤.")
                return []
            print(f"ğŸ“ MongoDBì—ì„œ {len(names)}ê°œ collection ì´ë¦„ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {names}")
            return names
        except Exception as e:
            print(f"âš ï¸ MongoDB collection ì´ë¦„ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}")
            return []

    def _chunking(self, md: str, file_uuid: str, file_name: str, collections: list[str]) -> dict[str, list[str]]:
        """
        ì œê³µëœ collection ì¤‘ ì²­í‚¹ì´ ì´ë¤„ì§€ì§€ ì•Šì€ collectionì´ ì¡´ì¬í•  ë•Œë§Œ ì²­í‚¹ì„ í•˜ê³ , ì²­í‚¹ì´ ì´ë¤„ì§€ì§€ ì•Šì€ collectionë§Œ í•„í„°ë§í•´ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.
        """

        is_file_exists_filter = Filter(
            must=[
                FieldCondition(
                    key="file_info.file_uuid",
                    match=MatchValue(value=file_uuid)
                )
            ]
        )

        collections_to_save = []

        for collection in collections:
            points, next_page = self.vector_db_client.scroll(
                collection_name=collection,
                scroll_filter=is_file_exists_filter,
                limit=1,
                with_payload=False,
            )

            if not points:
                collections_to_save.append(collection)

                if len(collections_to_save) > 0: # ë§Œì•½ ìƒˆë¡œ ì €ì¥í•´ì•¼ í•  collectionì´ ìˆë‹¤ë©´...
                    chunks = self.markdown_chunker.chunk_markdown_file(md, file_uuid, file_name, collections_to_save)

                    return {
                        "chunks": chunks,
                        "collections_to_save": collections_to_save
                    }
                
                else:
                    print("â˜‘ï¸ ìš”ì²­ëœ ëª¨ë“  collectionì— ì´ë¯¸ ì²­í‚¹ì´ ì´ë¤„ì ¸ ìˆìœ¼ë¯€ë¡œ ì²­í‚¹ì„ ìˆ˜í–‰í•˜ì§€ ì•Šê³  ë©”ì„œë“œë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    return None
            
        raise RuntimeError("â˜‘ï¸ ì œê³µëœ UUIDë¡œ ì‹œì‘í•˜ëŠ” PDFíŒŒì¼ì´ í”„ë¡œì íŠ¸ ë‚´ë¶€ PDF í´ë”ì— ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                

    def _indexing(self, chunks: list[dict], collections: list[str]):
        """
        ì²­í¬ë¥¼ ê°€ì ¸ì™€ì„œ ì¸ë±ì‹± í›„ ì €ì¥
        """
        self.elasticsearch_indexer.index_chunks(chunks, collections)


    def _embedding(self, chunks: list[dict], collections: list[str]):
        """
        ì²­í‚¹ì„ ê°€ì ¸ì™€ì„œ ì„ë² ë”© í›„ ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥
        """
        chunk_embedding_and_upsert(chunks, self.embedding_model, self.vector_db_client, collections)

    def _knowledge_graph(self, chunks: list[dict]):
        """
        ì²­í‚¹ì„ ê°€ì ¸ì™€ì„œ ì§€ì‹ ê·¸ë˜í”„ ì¶”ì¶œ ë° ì €ì¥
        """
        asyncio.run(Neo4jKnowledgeGraph().async_ingest_chunks(chunks))


    def run_pdf_ingestion_pipeline(self, md: str, file_uuid: str, file_name: str, collections: list[str]):
        """
        pdfë¥¼ ê°€ì ¸ì™€ì„œ ì²­í‚¹, ì¸ë±ì‹±, ì„ë² ë”©ê¹Œì§€ í•œ í•¨ìˆ˜ì—ì„œ í•˜ëŠ” í•¨ìˆ˜
        """

        try:
            chunks_and_collections = self._chunking(md, file_uuid, file_name, collections)

            if not chunks_and_collections:
                return

            self._indexing(chunks_and_collections["chunks"], chunks_and_collections["collections_to_save"])
            self._embedding(chunks_and_collections["chunks"], chunks_and_collections["collections_to_save"])
            self._knowledge_graph(chunks_and_collections["chunks"])
        except RuntimeError as e:
            print(e)
            return


if __name__ == "__main__":
    import glob
    from pathlib import Path

    pdf_ingestion_pipeline = PdfIngestionPipeline()

    # markdown_sample_data í´ë”ì˜ ëª¨ë“  .md íŒŒì¼ ì°¾ê¸°
    markdown_folder = "markdown_sample_data"
    md_files = glob.glob(f"{markdown_folder}/*.md")

    if not md_files:
        print(f"âš ï¸ '{markdown_folder}' í´ë”ì— .md íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"âœ… ì´ {len(md_files)}ê°œì˜ .md íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n")

        for i, markdown_file_path in enumerate(md_files, 1):
            print(f"{'='*80}")
            print(f"[{i}/{len(md_files)}] ì²˜ë¦¬ ì¤‘: {markdown_file_path}")
            print(f"{'='*80}\n")

            # íŒŒì¼ ì´ë¦„ì—ì„œ UUID ì¶”ì¶œ (ì˜ˆ: "Copy of 5bc0c676-018f-46de-bb0d-0103ff9c388c.md")
            file_name = Path(markdown_file_path).stem  # í™•ì¥ì ì œì™¸í•œ íŒŒì¼ëª…

            # "Copy of " ì œê±° ë° UUID ì¶”ì¶œ ë¡œì§
            if "Copy of " in file_name:
                file_uuid = file_name.replace("Copy of ", "").strip()
            else:
                file_uuid = file_name

            # íŒŒì¼ ì½ê¸°
            try:
                with open(markdown_file_path, "r", encoding="utf-8") as f:
                    md = f.read()

                # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                pdf_ingestion_pipeline.run_pdf_ingestion_pipeline(
                    md,
                    file_uuid,
                    file_name,
                    ["msds"],  # í•„ìš”ì— ë”°ë¼ ["msds", "tds"] ë“±ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥
                )

                print(f"\nâœ… [{i}/{len(md_files)}] ì™„ë£Œ: {file_name}\n")

            except Exception as e:
                print(f"\nâŒ [{i}/{len(md_files)}] ì‹¤íŒ¨: {file_name}")
                print(f"   ì˜¤ë¥˜: {e}\n")
                continue

        print(f"\n{'='*80}")
        print(f"ğŸ‰ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ! (ì´ {len(md_files)}ê°œ íŒŒì¼)")
        print(f"{'='*80}")