from dotenv import load_dotenv
import os
import re
from collections import defaultdict
from pymongo import MongoClient
from pymongo.errors import ConnectionFailure
from bson import ObjectId

load_dotenv()

USERNAME = os.getenv("MONGO_INITDB_ROOT_USERNAME")
PASSWORD = os.getenv("MONGO_INITDB_ROOT_PASSWORD")
HOST = os.getenv("MONGO_HOST")
PORT = int(os.getenv("MONGO_PORT"))

url = f"mongodb://{USERNAME}:{PASSWORD}@{HOST}:{PORT}/"

# mongodb ì—°ê²°
try:
    client = MongoClient(url)
    client.admin.command('ping')
    print("Successfully connected to MongoDB!")

except ConnectionFailure as e:
    print(f"MongoDB connection failed: {e}")

# dbì™€ collection ì„ íƒ
db = client['s307_db']
collection = db['s307_collection']



def save_markdown_to_mongodb(file_path: str) -> None:
    """
    .md íŒŒì¼ì„ ì½ì–´ì„œ MongoDBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        file_path: .md íŒŒì¼ì˜ ê²½ë¡œ

    Returns:
        None
    """
    # íŒŒì¼ëª… ì¶”ì¶œ (ê²½ë¡œì—ì„œ ë§ˆì§€ë§‰ ë¶€ë¶„)
    file_name = file_path.split("\\")[-1]
    
    try:
        # .md íŒŒì¼ ì½ê¸°
        with open(file_path, "r", encoding="utf-8") as file:
            markdown_content = file.read()
        
        # MongoDBì— ì €ì¥í•  ë¬¸ì„œ ìƒì„±
        markdown_object = {
            "file_name": file_name,
            "doc_type": "markdown",
            "context": markdown_content
        }
        
        # MongoDBì— ì €ì¥
        result = collection.insert_one(markdown_object)
        print(f"Inserted Markdown | File: {file_name} | ID: {result.inserted_id}")
        
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    except Exception as e:
        print(f"âŒ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

 
def normalize_variable_numbers(text: str, total_pages: int) -> str:
    """
    êµ¬ë¶„ì(/, ì˜, of, -)ë¥¼ ê°ì§€í•˜ì—¬ ê·¸ ì£¼ë³€ì˜ ìˆ«ìë¥¼ ë³€ìˆ˜í™”
    ì—°ë„ë‚˜ ë¶ˆí•„ìš”í•œ íŒ¨í„´ì€ ì œì™¸
    Args:
        text: ì •ê·œí™”í•  í…ìŠ¤íŠ¸
        total_pages: ì´ í˜ì´ì§€ ìˆ˜
    Returns:
        str: ì •ê·œí™”ëœ í…ìŠ¤íŠ¸
    """
    normalized_text = text
    
    # ì œì™¸í•  íŒ¨í„´ë“¤ (ì—°ë„, ë²„ì „ ë“±)
    exclude_patterns = [
        r'\d{4}',  # 4ìë¦¬ ì—°ë„ (2013, 2020 ë“±)
        r'\d{2}-\d{2}',  # 2ìë¦¬-2ìë¦¬ (13-08 ë“±)
        r'Copy\)',  # Copy)ë¡œ ëë‚˜ëŠ” ê²ƒ
        r'Printed',  # Printedê°€ í¬í•¨ëœ ê²ƒ
        r'Uncontrolled',  # Uncontrolledê°€ í¬í•¨ëœ ê²ƒ
    ]
    
    # ì œì™¸ íŒ¨í„´ì´ ìˆëŠ”ì§€ í™•ì¸
    for pattern in exclude_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            return text  # ì œì™¸ íŒ¨í„´ì´ ìˆìœ¼ë©´ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
    
    # êµ¬ë¶„ì íŒ¨í„´ë“¤: ìˆ«ì + (ê³µë°± í—ˆìš©) êµ¬ë¶„ì + ìˆ«ì
    patterns = [
        (r'(\d+)\s*/\s*(\d+)', r'n/{}'),          # 1/15, 1 / 15 -> n/15
        (r'(\d+)\s*-\s*(\d+)', r'n-{}'),          # 1-15, 1 - 15 -> n-15
        (r'(\d+)\s*ì˜\s*(\d+)', r'n ì˜ {}'),       # 1 ì˜ 11 -> n ì˜ 11
        (r'(\d+)\s+of\s+(\d+)', r'n of {}'),      # 1 of 11 -> n of 11
    ]
    
    for pattern, replacement in patterns:
        # íŒ¨í„´ì´ ë§¤ì¹˜ë˜ëŠ”ì§€ í™•ì¸
        if re.search(pattern, text, re.IGNORECASE):
            # ì²« ë²ˆì§¸ ìˆ«ìë¥¼ 'n'ìœ¼ë¡œ, ë‘ ë²ˆì§¸ ìˆ«ìë¥¼ total_pagesë¡œ ë³€ê²½ (ê³µë°± ì œê±°í•˜ì—¬ ìƒì„±)
            normalized_text = re.sub(pattern, replacement.format(total_pages), normalized_text, flags=re.IGNORECASE)
    
    return normalized_text



def get_zone(page, which: str, ratio: float = 0.3):
    """
    ê° í˜ì´ì§€ì˜ ìƒë‹¨ 30% ë˜ëŠ” í•˜ë‹¨ 30% ì˜ì—­ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    Args:
        page: í˜ì´ì§€ ë‚´ìš©
        which: 'header' ë˜ëŠ” 'footer'
        ratio: ì˜ì—­ ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.3)
    Returns:
        zone: ì¶”ì¶œëœ ì˜ì—­ ë‚´ìš©
    """
    if which == 'header':
        end = max(1, int(len(page) * ratio))
        return page[:end]
    else:
        count = max(1, int(len(page) * ratio))
        start = max(0, len(page) - count)
        return page[start:]


def collect_common(all_pages, which: str, total_pages: int, exclude_first: bool):
    """
    ìƒ/í•˜ë‹¨ ìœˆë„ìš°ì—ì„œ í˜ì´ì§€ë³„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì§‘í•´ ëª¨ë“  ìœ íš¨ í˜ì´ì§€ì— ì¡´ì¬í•˜ëŠ” í•­ëª©ë§Œ ë°˜í™˜
    Args:
        all_pages: ëª¨ë“  í˜ì´ì§€ ë‚´ìš©
        which: ì¶”ì¶œ ëŒ€ìƒ. 'header' ë˜ëŠ” 'footer'
        total_pages: ì´ í˜ì´ì§€ ìˆ˜
        exclude_first: ì²« í˜ì´ì§€ ì œì™¸ ì—¬ë¶€
    Returns:
        common: ëª¨ë“  ìœ íš¨ í˜ì´ì§€ì— ì¡´ì¬í•˜ëŠ” ê³µí†µ í•­ëª©ë§Œ ì¶”ì¶œ(header ë˜ëŠ” footer)
    """
    first_seen = {}
    page_count = defaultdict(int)
    effective_pages = (total_pages - 1) if exclude_first else total_pages

    for page_idx, page in enumerate(all_pages):
        zone = get_zone(page, which)
        # ìµœì´ˆ ë“±ì¥ ì¢Œí‘œ ê¸°ë¡
        for local_idx, line in enumerate(zone):
            text = line.strip()
            if not text:
                continue
            key = normalize_variable_numbers(text, total_pages) if which == 'footer' else text
            first_seen.setdefault(key, (page_idx, local_idx if which == 'header' else (len(page) - len(zone) + local_idx)))

        # ë¹ˆë„ ì§‘ê³„ (ì²« í˜ì´ì§€ ì œì™¸ ê°€ëŠ¥)
        if exclude_first and page_idx == 0:
            continue
        uniq = set()
        for line in zone:
            text = line.strip()
            if not text:
                continue
            key = normalize_variable_numbers(text, total_pages) if which == 'footer' else text
            uniq.add(key)
        for key in uniq:
            page_count[key] += 1

    # ëª¨ë“  ìœ íš¨ í˜ì´ì§€ì— ë“±ì¥í•œ í•­ëª©ë§Œ
    commons = [k for k, c in page_count.items() if c == effective_pages]
    # ìµœì´ˆ ë“±ì¥ ì¢Œí‘œ ìˆœìœ¼ë¡œ ì •ë ¬í•´ ë¬¸ì„œ ìˆœì„œ ë³´ì¡´
    return [k for k in sorted(commons, key=lambda x: first_seen.get(x, (9999, 9999)))]

def get_header_footer_info(context: str) -> dict:
    """
    Header/Footer ë¶„ì„ì„ ìˆ˜í–‰í•˜ì—¬ ê³µí†µ íŒ¨í„´ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        context: ì €ì¥ëœ context ë¬¸ìì—´

    Returns:
        dict: {
            'header': ê³µí†µ header í…ìŠ¤íŠ¸,
            'footer': ê³µí†µ footer í…ìŠ¤íŠ¸ (ìˆ«ìëŠ” nìœ¼ë¡œ í‘œì‹œ),
            'content': headerì™€ footerë¥¼ ì œì™¸í•œ ìˆœìˆ˜ content í…ìŠ¤íŠ¸ë“¤
        }
    """
    # 1. í˜ì´ì§€ë³„ë¡œ ë¶„í•  (>>> page X ~ >>> pend êµ¬ì¡°)
    # >>> page x ì™€ >>> pend ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ ë‚´ìš© ì¶”ì¶œ
    page_pattern = r'>>> page \d+\n(.*?)\n>>> pend'
    page_matches = re.findall(page_pattern, context, re.DOTALL)

    # ì´ˆê¸°í™” ì²˜ë¦¬
    if not page_matches:
        return {'header': '', 'footer': '', 'content': ''}

    all_pages = []
    for page_content in page_matches:
        page_lines = page_content.strip().split('\n')
        all_pages.append(page_lines)
    
    # header, footer ì´ˆê¸°í™”(ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”)
    common_headers = []
    common_footers = []
    total_pages = len(all_pages)

    if total_pages == 1:
        # 1í˜ì´ì§€: Header/Footer ë¡œì§ ìŠ¤í‚µ
        pass
    elif total_pages == 2:
        # 2í˜ì´ì§€: ë‘ í˜ì´ì§€ ì§ì ‘ ë¹„êµ
        common_headers = collect_common(all_pages, 'header', total_pages, exclude_first=False)
        common_footers = collect_common(all_pages, 'footer', total_pages, exclude_first=False)
    else:
        # 3í˜ì´ì§€ ì´ìƒ: ì²« í˜ì´ì§€ ì œì™¸ í›„ ëª¨ë“  ìœ íš¨ í˜ì´ì§€ì— ì¡´ì¬í•˜ëŠ” í•­ëª©ë§Œ
        common_headers = collect_common(all_pages, 'header', total_pages, exclude_first=True)
        common_footers = collect_common(all_pages, 'footer', total_pages, exclude_first=True)
    
    print(f"ë™ì‹œ ë¶„ì„ ê²°ê³¼ - Header: {len(common_headers)}ì¤„, Footer: {len(common_footers)}ì¤„")
    
    # FooterëŠ” ì´ë¯¸ ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ë¡œ ìˆ˜ì§‘ë¨
    total_pages = len(page_matches)
    processed_footers = list(common_footers)
    
    # Header í…ìŠ¤íŠ¸ ì¡°í•©
    header_text = '\n'.join(common_headers) if common_headers else ""
    
    # Footer í…ìŠ¤íŠ¸ ì¡°í•©
    footer_text = '\n'.join(processed_footers) if processed_footers else ""
    
    # Content ì¶”ì¶œ (ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ headerì™€ footerë¥¼ ì œê±°í•œ ë‚´ìš©, êµ¬ë¶„ì ìœ ì§€)
    content_text = context
    
    # Header ì œê±° (í•œ ë²ˆì˜ ìŠ¤ìº”ìœ¼ë¡œ ì²˜ë¦¬)
    if common_headers:
        lines = content_text.split('\n')
        header_set = set(common_headers)
        filtered_lines = []
        for line in lines:
            if line.strip() in header_set:
                continue
            filtered_lines.append(line)
        content_text = '\n'.join(filtered_lines)
    
    # Footer ì œê±° (ì •ê·œí™” ë¹„êµ, í•œ ë²ˆì˜ ìŠ¤ìº”)
    if common_footers:
        lines = content_text.split('\n')
        footer_set = set(common_footers)
        filtered_lines = []
        for line in lines:
            normalized_line = normalize_variable_numbers(line.strip(), total_pages)
            if normalized_line in footer_set:
                continue
            filtered_lines.append(line)
        content_text = '\n'.join(filtered_lines)
    
    return {
        'header': header_text,
        'footer': footer_text,
        'content': content_text
    }


def save_processing_to_mongodb(file_name: str, header_footer_info: dict) -> None:
    """
    Header/Footer ë¶„ì„ ê²°ê³¼ë¥¼ processing ê°ì²´ë¡œ MongoDBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        file_name: ì›ë³¸ íŒŒì¼ëª…
        header_footer_info: get_header_footer_info()ì˜ ë°˜í™˜ê°’ (dict)
    """
    try:
        # MongoDBì— ì €ì¥í•  processing ê°ì²´ ìƒì„±
        processing_object = {
            "file_name": file_name,
            "doc_type": "processing",
            "header": header_footer_info['header'],
            "footer": header_footer_info['footer'],
            "content": header_footer_info['content']
        }
        
        # MongoDBì— ì €ì¥
        result = collection.insert_one(processing_object)
        print(f"âœ… Processing ê°ì²´ ì €ì¥ ì™„ë£Œ | File: {file_name} | ID: {result.inserted_id}")
        
    except Exception as e:
        print(f"âŒ Processing ê°ì²´ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")




if __name__ == "__main__":
    # # íŠ¹ì • ObjectIdë¡œ ë¬¸ì„œ ì‚­ì œ
    # target_id = ObjectId("68f8e4b6efb465991543e50e")
    # # íŠ¹ì • IDì˜ ë¬¸ì„œ ì‚­ì œ
    # result = collection.delete_one({"_id": target_id})
    # if result.deleted_count > 0:
    #     print(f"âœ… ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ: {target_id}")
    # else:
    #     print(f"âŒ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {target_id}")



    # t1,t2,t3,t4 í´ë”ì˜ output.md íŒŒì¼ ì²˜ë¦¬
    t1_md_path = r"C:\Users\SSAFY\Desktop\S13P31S307\parsing\testmds\t1\44-1206-SDS11757.md"
    
    # 1. .md íŒŒì¼ì„ MongoDBì— ì €ì¥
    print("=== 1ë‹¨ê³„: .md íŒŒì¼ ì €ì¥ ===")
    save_markdown_to_mongodb(t1_md_path)
    
    # 2. ì €ì¥ëœ ë¬¸ì„œ ì°¾ê¸°
    print("\n=== 2ë‹¨ê³„: ì €ì¥ëœ ë¬¸ì„œ í™•ì¸ ===")
    file_name = t1_md_path.split("\\")[-1]  # Windows ê²½ë¡œì´ë¯€ë¡œ \\ë¡œ ë¶„í• 
    saved_doc = collection.find_one({"file_name": file_name, "doc_type": "markdown"})
    
    if saved_doc:
        print(f"âœ… ë¬¸ì„œ ì°¾ìŒ: {file_name}")
        print(f"ë¬¸ì„œ ID: {saved_doc['_id']}")
        print(f"Context ê¸¸ì´: {len(saved_doc['context'])} ë¬¸ì")
        
        # 3. Header/Footer ë¶„ì„
        print("\n=== 3ë‹¨ê³„: Header/Footer ë¶„ì„ ===")
        header_footer_info = get_header_footer_info(saved_doc['context'])
        
        # print(f"\nğŸ“„ Header ({len(header_footer_info['header'])} ë¬¸ì):")
        # print(header_footer_info['header'])
        
        # print(f"\nğŸ“„ Footer ({len(header_footer_info['footer'])} ë¬¸ì):")
        # print(header_footer_info['footer'])
        
        # print(f"\nğŸ“„ Content (ì²˜ìŒ 100ì):")
        # content_preview = header_footer_info['content'][:100]
        # print(content_preview + "..." if len(header_footer_info['content']) > 100 else content_preview)
        
        # print(f"\nğŸ“Š í†µê³„:")
        # print(f"Header ê¸¸ì´: {len(header_footer_info['header'])} ë¬¸ì")
        # print(f"Footer ê¸¸ì´: {len(header_footer_info['footer'])} ë¬¸ì") 
        # print(f"Content ê¸¸ì´: {len(header_footer_info['content'])} ë¬¸ì")
        
        # 4. Processing ê°ì²´ë¡œ MongoDBì— ì €ì¥
        print("\n=== 4ë‹¨ê³„: Processing ê°ì²´ ì €ì¥ ===")
        save_processing_to_mongodb(file_name, header_footer_info)
        
    else:
        print(f"âŒ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_name}")